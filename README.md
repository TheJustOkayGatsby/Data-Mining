This is a selection of exercises from the Data Mining Module. This was a main focal point of the degree course of study and expanded on the first half of the computational keystone: Machine Learning. Machine Learning exercises may be found in seperate repo.

Models used in this module:
1) **Naive Bayes**
2) **SVC**: Support Vector Classification
3) **SVM**: Support Vector Machine
4) **PCA**: Principal Component Analysis
5) **UMAP**: Uniform Manifold Approximation and Projection
6) Decision Trees, sort of
7) **Random Forest**
8) **AdaBoost**
9) **Gradient Boosting**
10) **Logistic Regression**
11) **MLP**: MultiLayer Perceptron
12) **K Means**
13) **DB Scan**

Topics covered within this module: 
1) **Text Mining**

   This converted unstructured text to a cleaned text which could be mined for tokens to make predictions with. Supervised and unsupervised.
   Using **BeautifulSoup**
   Using **Natural Language ToolKit(NLTK)**
   Running **classification**, **clustering** and **regression** analysis on featurized texts

  3) **Neural Nets**, in theory and in deployment
    - **MultiLayer Perceptron**
 
  4) **Clustering Analysis**
     - via **PCA, Kmeans, DBScan**
    
  5) Graph Mining
     - This was the most robust part of the module, and included **pathfinding** and **centrality** algorithms, all of which were solved by hand prior to introducing code.
     - Taught methods:
       - **Dijkstra**'s
       - **Ford Fulkerson**
       - **Betweenness** Centrality
       - **Closeness** Centrality
       - **Degree** Centrality
